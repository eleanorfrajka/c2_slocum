{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51dc8981",
   "metadata": {},
   "source": [
    "# Notebook to test edits for grid_glider_data.py file\n",
    "\n",
    "THIS IS ONLY FOR TESTING CODE.  For operational use, start with **grid_glider_data.py**\n",
    "\n",
    "Script to grid glider (slocum) data and make various calculations on the profiles.\n",
    "\n",
    "1. Works locally on the processed netcdf glider time series *o2.nc\n",
    "2. Grid glider data\n",
    "3. Calculate MLD\n",
    "\n",
    "Runs offline, using the netcdf files created in process_glider_tseries.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df001618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "# Own packages of code\n",
    "from setdir import *\n",
    "from parseglider import *\n",
    "from calc_glider import *\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779f8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice of grid interval (pressure in dbar)\n",
    "dp=10\n",
    "\n",
    "## CHANGE TO A CONFIG FILE WITH USER DEFINABLE PARAMETERS\n",
    "# Slocum gliders: A dictionary with the key as the serial number ('unit_398') \n",
    "# and then the plain text name, \"Churchill\"\n",
    "glider_names = {\n",
    "    'unit_398': 'Churchill',\n",
    "    'unit_409': 'Grease',\n",
    "}\n",
    "\n",
    "# List of glider serial numbers for API\n",
    "unit_list = [(k) for k in glider_names.keys()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd09a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed to ../01-data/03-processed/unit_409_20220311_bin10m.nc\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# GRIDDING DATA AND CALCULATIONS ON THE GRIDDED DATA \n",
    "# Grid the data and make some calculations on the gridded data (MLD)\n",
    "#----------------------------------------------------------------------\n",
    "for uname in unit_list:\n",
    "    fname = uname+'*_data_o2.nc'\n",
    "    \n",
    "    # Extract a list with the names of existing interim data files\n",
    "    existing_files = glob.glob(cat_interim_path(fname))\n",
    "    \n",
    "    # Check whether there are any files\n",
    "    if len(existing_files) > 0:\n",
    "        # Extract the most recent filename\n",
    "        existing_files = sorted(existing_files)\n",
    "        latest_file = existing_files[-1]\n",
    "        \n",
    "        # Open the dataset\n",
    "        data_ds = xr.open_dataset(latest_file)\n",
    "         \n",
    "        if 0:\n",
    "            # Check whether a gridded file has already been created\n",
    "            # Not yet implemented\n",
    "            proc_files = glob.glob(cat_interim_path(fname))\n",
    "            if not len(proc_files) > 0:\n",
    "                print('No processed files for that glider')\n",
    "   \n",
    "        #--------------------------------------------------------------\n",
    "        # Grid data onto a regular pressure grid (intervals given by dp)\n",
    "        # - Grid data into a 2d matrix against profile index & pressure grid \n",
    "        #    NOTE: Gridding is rough and *not* science quality\n",
    "        #--------------------------------------------------------------\n",
    "        grid_ds = bin_dp(data_ds, data_ds.attrs['Serial number'], dp)\n",
    "       \n",
    "        # EFW: I think closing these helps with file management & permission \n",
    "        # denied problems? \n",
    "        data_ds.close()\n",
    "\n",
    "\n",
    "        #------------------------------------------\n",
    "        # ADD EXTRA COORDINATES (length divenum)\n",
    "        #------------------------------------------\n",
    "        # Simplifies plotting later to plot against time or distance\n",
    "        mtime = grid_ds.time.mean(dim='pressure').values\n",
    "        mlon = grid_ds.m_lon.mean(dim='pressure').values\n",
    "        mlat = grid_ds.m_lat.mean(dim='pressure').values\n",
    "\n",
    "        # Interpolate over lat and long values\n",
    "        divenum = grid_ds.divenum.values\n",
    "\n",
    "        # Lon\n",
    "        idxnan = (~np.isnan(mlon))\n",
    "        divenum_nonnan = divenum[idxnan]\n",
    "        mlon_nonnan = mlon[idxnan]\n",
    "        flon = interp1d(divenum_nonnan, mlon_nonnan,\n",
    "                        kind='linear', fill_value=\"extrapolate\")\n",
    "        mlon_full = flon(divenum)\n",
    "\n",
    "        # Lat\n",
    "        idxnan = (~np.isnan(mlat))\n",
    "        divenum_nonnan = divenum[idxnan]\n",
    "        mlat_nonnan = mlat[idxnan]\n",
    "        flat = interp1d(divenum_nonnan,mlat_nonnan,\n",
    "                        kind='linear', fill_value=\"extrapolate\")\n",
    "        mlat_full = flat(divenum)\n",
    "\n",
    "        # Calculate distances from the interpolated lat/lon positions\n",
    "        dist_km = gsw.distance(mlat_full, mlon_full, 0, axis=0)/1000\n",
    "        dist_km_pad = np.append(0, dist_km)\n",
    "        # Cumsum is a problem, need to do something about NaN?\n",
    "        dist_along_track = np.cumsum(dist_km_pad)\n",
    "\n",
    "        # Create data array versions\n",
    "        DAT_2 = xr.DataArray(dist_along_track, \n",
    "                             coords={\"divenum\": grid_ds.divenum},\n",
    "                             attrs=dict(long_name=\"Distance\", units=\"km\"))\n",
    "        TIME_2 = xr.DataArray(mtime, \n",
    "                              coords={\"divenum\": grid_ds.divenum},\n",
    "                             attrs=dict(long_name=\"Date\"))\n",
    "        LAT_2 = xr.DataArray(mlat_full, \n",
    "                             coords={\"divenum\": grid_ds.divenum},\n",
    "                            attrs=dict(long_name=\"Latitude\"))\n",
    "        LON_2 = xr.DataArray(mlon_full, \n",
    "                             coords={\"divenum\": grid_ds.divenum},\n",
    "                            attrs=dict(long_name=\"Longitude\"))\n",
    "\n",
    "\n",
    "\n",
    "        grid_ds[\"dist_along_track\"] = DAT_2\n",
    "        grid_ds[\"timevec\"] = TIME_2\n",
    "        grid_ds[\"lonvec\"] = LON_2\n",
    "        grid_ds[\"latvec\"] = LAT_2\n",
    "\n",
    "        # Change the variables to coordinates\n",
    "        grid_ds = grid_ds.set_coords(['dist_along_track','timevec',\n",
    "                                      'lonvec','latvec'])\n",
    "\n",
    "        #-------------------------------------------------\n",
    "        # Calculate mixed layer depth\n",
    "        #-------------------------------------------------\n",
    "        grid_ds = calc_MLD(grid_ds)\n",
    "\n",
    "        #-------------------------------------------------\n",
    "        # Save gridded to 01-data/03-processed/*_bin10m.nc\n",
    "        #-------------------------------------------------\n",
    "        # Filename as 'unit_409_YYYYMMDD_bin10m.nc'\n",
    "        uname = data_ds.attrs['Serial number']\n",
    "        maxtimestr = data_ds.attrs['End Time']\n",
    "        outfile = uname+'_'+maxtimestr+'_bin10m.nc'\n",
    "        print('Saving processed to '+cat_proc_path(outfile))\n",
    "        grid_ds.to_netcdf(cat_proc_path(outfile), mode='w')\n",
    "        \n",
    "        # EFW: I think closing these helps with file management & permission \n",
    "        # denied problems? \n",
    "        grid_ds.close()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f40e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b117b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 profiles and 61 Argo floats found in the last 2 months\n",
      "Profiles Real time:220\n",
      "Profiles Delayed mode:5\n",
      "Profiles Adjusted:179\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Load and save Argo data using argopy\n",
    "# Calculate Argo MLD\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "# Instantiate default argopy data fetcher\n",
    "from argopy import DataFetcher as ArgoDataFetcher\n",
    "argo_loader = ArgoDataFetcher()\n",
    "\n",
    "date_now = datetime.datetime.now()\n",
    "date_1mth = date_now+datetime.timedelta(days=-2*30)\n",
    "# Request data for a specific space/time domain\n",
    "ag_points = argo_loader.region([-66,-44,45,68,0,1000,\n",
    "                                date_1mth.strftime(\"%Y-%m-%d\"),\n",
    "                                date_now.strftime(\"%Y-%m-%d\")]).to_xarray() # 45-68N - 66-44W\n",
    "# Save by profiles\n",
    "ag = ag_points.argo.point2profile()\n",
    "# TEOS-10 variables\n",
    "ag.argo.teos10(['SA', 'CT', 'PV'])\n",
    "\n",
    "# DATA_MODE ( R for real time data, D for delayed mode data, A for real time adjusted data )\n",
    "DATA_MODE = ['Real time','Delayed mode','Adjusted']\n",
    "print( str(ag.N_PROF.values.shape[0]) + ' profiles and ' + str(np.unique(ag.PLATFORM_NUMBER.values).shape[0]) + ' Argo floats found in the last 2 months')\n",
    "for i in range(len(DATA_MODE)):\n",
    "    print('Profiles ' + DATA_MODE[i] + ':' + str( (np.where( ag.DATA_MODE.values == DATA_MODE[i][0] )[0]).shape[0]) )\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Calculate MLD\n",
    "#-------------------------------------------------\n",
    "ag['MLD']=xr.DataArray( np.full(ag.N_PROF.shape[0],np.nan), coords={\"N_PROF\": ag.N_PROF})\n",
    "for i in range(ag.N_PROF.shape[0]):\n",
    "    if np.nanmax(ag.PRES.values[i,:])>10:\n",
    "        ag['MLD'][i]=MLD_i( gsw.sigma0(ag.SA.values[i,:],ag.CT.values[i,:]), gsw.z_from_p(ag.PRES.values[i,:],ag.LATITUDE.values[i]))\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Save Argo in nc file\n",
    "#-------------------------------------------------\n",
    "outfile = 'Argo_'+ date_now.strftime(\"%Y-%m-%d\") +'.nc'\n",
    "if Path(cat_proc_path(outfile)).is_file()==0:\n",
    "    print('Saving Argo file to '+cat_proc_path(outfile))\n",
    "    ag.to_netcdf(cat_proc_path(outfile), mode='w')\n",
    "    ag.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b19ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b3cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4efabe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db624b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_c2slocum]",
   "language": "python",
   "name": "conda-env-env_c2slocum-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
