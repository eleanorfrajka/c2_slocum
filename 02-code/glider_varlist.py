"""
EFW - this file isn't tidied up (extra variables and packages)

It's only purpose is to download the variable list from the api - removed from update_data.py
"""

import numpy as np
import pandas as pd
import xarray as xr
import matplotlib.pyplot as plt
import os
import glob
import datetime as dt
import requests
import json
from io import StringIO
import ast # To handle the string conversion when loading json filee
from pathlib import Path
# Own packages of code
from setdir import *
from parseglider import *
from plotglider import *
from calc_oxy import *


## CHANGE TO A CONFIG FILE WITH USER DEFINABLE PARAMETERS
# Slocum gliders: A dictionary with the key as the serial number ('unit_398') 
# and then the plain text name, "Churchill"
glider_names = {
    'unit_398': 'Churchill',
}

#    'unit_409': 'Grease',


sensor_sn = {
    'unit_398': {"optode S/N": "232"},
    'unit_409': {"optode S/N": "268"},
}
# Dictionary keys MUST match the serial number format used in the API.  

# Choose a start date in YYYY-MM-DD.  
# Earliest valid data for TERIFIC was 2021-12-12, but there were some in-air 
# measurements before
mission_startdate = '2021-12-12'

# Choose variable names
# Check the Slocum master data list 8.2 for a range of options
var_physics = ['sci_water_pressure', 'sci_water_temp', 'sci_water_cond',
            'derived_salinity', 'derived_potential_density', 'derived_potential_temperature',
           ]

var_other = ['m_final_water_vx', 'm_final_water_vy',
             'm_final_water_vx_at_surface',
             'm_final_water_vy_at_surface',
             'm_water_vx', 'm_water_vy',
             'm_gps_lon', 'm_gps_lat',
             'm_lat', 'm_lon',
            ]
             
var_oxy = ['sci_oxy4_oxygen',
           'sci_oxy4_calphase',
           'sci_oxy4_temp',
    #'sci_oxy4_c1amp',
    #       'sci_oxy4_c1rph',
    #       'sci_oxy4_c2amp',
    #       'sci_oxy4_c2rph',
#           'sci_oxy4_is_installed',
#           'sci_oxy4_rawtemp',
#           'sci_oxy4_saturation',
#           'sci_oxy4_tcphase',
          ]

# Wetlabs on Unit_398: these parameters seem to work (bb2flsv9)
# Wetlabs on unit_409: these parameters seem to work (flbbcd)
# POSSIBLE TO JUST ADD MORE VARIABLES AND THEY WILL BE REMOVED LATER
# BUT PROBABLY SLOWS THINGS DOWN...
var_bio = ['sci_bb2flsv9_b532_scaled', # units ug/l  - blue?? or green
            'sci_bb2flsv9_b700_scaled', # units ug/l - red
            'sci_bb2flsv9_chl_scaled', # units ug/l
            'sci_bb2flsv9_b532_sig', # units ug/l  - blue?? or green
            'sci_bb2flsv9_b700_sig', # units ug/l - red
            'sci_bb2flsv9_chl_sig', # units ug/l
            'sci_bb2flsv9_b532_ref', # units ug/l  - blue?? or green
            'sci_bb2flsv9_b700_ref', # units ug/l - red
            'sci_bb2flsv9_chl_ref', # units ug/l
            'sci_flbbcd_cdom_units', # ppb - 409
            'sci_flbbcd_chlor_units', # ug/l
            'sci_flbbcd_bb_units', # ??? is this blue backscatter?
            'sci_flbbcd_cdom_scaled', # ppb - 409
            'sci_flbbcd_chlor_scaled', # ug/l
            'sci_flbbcd_bb_scaled', # ??? is this blue backscatter?
            'sci_flbbcd_cdom_sig', # ppb - 409
            'sci_flbbcd_chlor_sig', # ug/l
            'sci_flbbcd_bb_sig', # ??? is this blue backscatter?

          ]


# Some details for the attributes in the netcdf file.
platform_type = 'slocum' # Must be in this format to work with the API
project_name = 'TERIFIC'
institution_name = 'National Oceanography Centre, UK'

# Choice of grid interval (pressure in dbar)
dp=10

# Choose name for new DataArrays to index the profiles:
idxname = 'profile_index'
# Choose name for new DataArray for pressure in dbar:
presname = 'pressure_dbar'



#==================================================================================
# Should not need to edit below here
#==================================================================================

#----------------------------------------------------------------------------
# Check that output directories exist - if not, then exit
# ?? Is there a better way to do this with error handling? [EFW]
#----------------------------------------------------------------------------
outpath = cat_interim_path('')
if not os.path.isdir(outpath):
    print('Directory not found: '+outpath+'    -- EXITING')
    sys.exit(1)
outpath = cat_proc_path('')
if not os.path.isdir(outpath):
    print('Directory not found: '+outpath+'    -- EXITING')
    sys.exit(1)

#----------------------------------------------------------------------------
# Token and API choices/formatting.
#----------------------------------------------------------------------------
# https://api.c2.noc.ac.uk/charon/tokens/issue
#
# Need to copy and paste the token you generated by logging in at the website
# above into the file 02-code/myToken.txt
with open("myToken.txt", "r") as myfile:
    myToken = myfile.read().replace('\n', '')
    
from requests.structures import CaseInsensitiveDict
headers = CaseInsensitiveDict()
headers["Accept"] = "application/json"
headers["Authorization"] = f'Bearer {myToken}'

# List of glider serial numbers for API
unit_list = [(k) for k in glider_names.keys()]

# URL for the data
api_root = 'https://api.c2.noc.ac.uk/'

# Platform type for API
platform = platform_type

# Format the time string
time_strf = '%Y%m%d'

# Used to chop data before this date
tstart = pd.Timestamp(mission_startdate+'T00')

# Change this to a later value to download only a subset of the data
download_startdate = mission_startdate+'T00%3A00%3A00'

# Date created (for attributes in netcdf file)
date_created = dt.datetime.now().strftime(time_strf)



#----------------------------------------------------------------------------
# API choices specific for data (not positioning)
#----------------------------------------------------------------------------
# Choice of API website for glider sensor data: timeseries/
api_choice = 'timeseries/observations/'

# Specify format for downloaded file:
# Using csv_combined_transposed rather than csv_combined since it seems to 
# help with getting all the data (not just when the wetlabs was on)
format_choice = 'csv_combined_transposed?'

# Format the variable list for the API
var_list = var_physics+var_bio+var_oxy+var_other
var_str = ''
for i in var_list:
    var_str = var_str+'variable='+i+'&'




#----------------------------------------------------------------------------
# Save list of all the variables output from the API, using /timeseries/meta/variables/
#----------------------------------------------------------------------------

for uname in unit_list:
    ursl_var_list='https://api.c2.noc.ac.uk/timeseries/meta/variables?platform_serial='+str(uname)+'&platform_type=slocum'
    resp_var_list = requests.get(ursl_var_list, headers=headers)
    
    # Check the response code 
    # (200 is good.  If you get something else, token may need refreshing)
    if not resp_var_list.status_code==200:
        print(uname+' - [ resp '+str(resp_var_list.status_code)+' ] '\
              'Cannot access data - May need to refresh token? or check URL variable')
    else:
        print(uname+' - [ resp '+str(resp_var_list.status_code)+' ] '\
              'Good response code - parsing var list')
        
        data_folder='03-results/variables_list/'
        filename=uname+'_var_list.txt'
        filename_sci=uname+'_var_list_sci.txt' # Save only sci & derived data
        if not Path('../'+data_folder).exists():
            Path('../'+data_folder).mkdir()
    
        var_list = resp_var_list.content.decode("utf-8")[1:-2].split(',') # split converts str to list, [1:-2] removes square brackets
        var_list2 = [k.strip().replace('"', '') for k in var_list ] # remove leading space and double quotes
        var_list_sci=[k for k in var_list2 if (k[:3]=='sci') | (k[:2]=='de')] # keep only sci & derived data
        
        np.savetxt( cat_data_path(data_folder,filename), var_list2, delimiter =",",fmt ='%s')
        np.savetxt( cat_data_path(data_folder,filename_sci), var_list_sci, delimiter =",",fmt ='%s')
    
    

#--------------------------------------------------------------
# DATA PROCESSING:
# - Remove non-existent sensors (empty DataArrays)
# - Assign a profile index to separate dives and climbs
# - Grid data into a 2d matrix against profile index & pressure grid 
#    NOTE: Gridding is rough and *not* science quality
#
# Save new files to 01-data/03-processed/ as
#   UNIT_data_YYYYMMDD.nc for the full data as a vector
#   UNIT_bin10m.nc for the rough gridded data.
#--------------------------------------------------------------
for uname in unit_list:
    fname = uname+'_data_*nc'
    
    # Extract a list with the names of existing raw data files
    existing_files = glob.glob(cat_raw_path(fname))

    # Check whether there are any files
    if len(existing_files) > 0:
        # Extract the end date from the filename
        existing_files = sorted(existing_files)
        latest_file = existing_files[-1]
        
        # Open the dataset
        data_ds = xr.open_dataset(latest_file)
        
        #--------------------------------------------------------------
        # Assign profile index (separates dives and climbs)
        #--------------------------------------------------------------
        # where 20.0 means the twentieth dive (downward profile)
        # and 20.5 is the twentieth climb (upward profile)
        data_ds, _, _ = dive_index(data_ds, presname, idxname)
         
        if 0:
            # Check whether a gridded file has already been created
            # Not yet implemented
            proc_files = glob.glob(cat_interim_path(fname))
            if not len(proc_files) > 0:
                print('No processed files for that glider')
        
        #--------------------------------------------------------------
        # Clean up Xarray dataset:
        # Remove data ararys that have no non-nan values
        #--------------------------------------------------------------
        for varname in data_ds.keys():
            data_val = data_ds[varname].values
            num = np.count_nonzero(~np.isnan(data_val))
            if num==0:
                data_ds = data_ds.drop(varname)
        
        # Save the result to processed data, 01-data/03-processed/*_data_YYYYMMDD.nc
        fname1 = os.path.basename(latest_file)
        data_ds.to_netcdf(cat_proc_path(fname1))
        
        
        #--------------------------------------------------------------
        # Calculate oxygen (maybe this should be elsewhere)
        # but I need it before gridding (or need a function to grid
        # individual extra variables)
        #--------------------------------------------------------------
        #sensor_sn = {
        #    'unit_398': {"optode S/N": "232"},
        #    'unit_409': {"optode S/N": "268"},
        #}
        sensorsn1 = sensor_sn[uname]
        data_ds = data_ds.assign_attrs(sensorsn1)
        print(data_ds)
        data_ds = calc_o2conc_cal(data_ds)


        #--------------------------------------------------------------
        # Grid data onto a regular pressure grid (intervals given by dp)
        # Saves output to 01-data/03-processed/*_bin10m.nc
        #--------------------------------------------------------------
        data_ds.to_netcdf(cat_proc_path('save_test.nc'),mode='w')
        grid_ds = bin_dp(data_ds, 'unit_398', dp)
        grid_ds.to_netcdf(cat_proc_path('save_test_grid.nc'), mode='w')
        tmp = data_ds.attrs['Serial number']
        print(tmp)
        name1 = cat_proc_path(grid_ds.attrs['Serial number']+'_bin10m.nc')
        print(name1)
        grid_ds = bin_dp(data_ds, data_ds.attrs['Serial number'], dp)
        grid_ds.to_netcdf(cat_proc_path(grid_ds.attrs['Serial number']+'_bin10m.nc'), mode='w')
        
