{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428ca4b2-e930-4925-ba9e-7d77d5fcd896",
   "metadata": {},
   "source": [
    "# Notebook to test edits for download_raw_data.py file\n",
    "\n",
    "THIS IS ONLY FOR TESTING CODE.  For operational use, start with **download_raw_data.py**\n",
    "\n",
    "Script to download glider (slocum) data from the C2/API web services at NOC MARS.  \n",
    "\n",
    "Creates netcdf files for later processing of glider data\n",
    "\n",
    "Requires an up-to-date token in the text file: myToken.txt\n",
    "\n",
    "FEATURE TO ADD:\n",
    "Change to user-defined inputs in a config file including\n",
    "- the glider to download\n",
    "- variables to include (and serial numbers for sensors?)\n",
    "- mission start date\n",
    "- more?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67044f0-59d7-4f4b-8267-405124a756d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# March 2022 - reduced packages (no plotting or parsing of the glider)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "import datetime as dt\n",
    "import requests\n",
    "import json\n",
    "from io import StringIO\n",
    "import ast # To handle the string conversion when loading json filee\n",
    "from pathlib import Path\n",
    "# Own packages of code\n",
    "from setdir import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cdfed1b-02ee-404d-874f-9ee2dcbbd9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "## ISSUE: CHANGE TO A CONFIG FILE WITH USER DEFINABLE PARAMETERS\n",
    "#----------------------------------------------------------------------------\n",
    "# Slocum gliders: A dictionary with the key as the serial number ('unit_398') \n",
    "# and then the plain text name, \"Churchill\"\n",
    "glider_names = {\n",
    "    'unit_398': 'Churchill',\n",
    "    'unit_409': 'Grease',\n",
    "}\n",
    "\n",
    "# Note: Dictionary keys in glider_names MUST match the serial number format \n",
    "# used in the c2 API.  \n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Specify the startdate for the download\n",
    "# ISSUE: In a future edit, only the recent data could be downloaded (rather than \n",
    "# redownloading the whole dataset each time)\n",
    "#----------------------------------------------------------------------------\n",
    "# Choose a start date in YYYY-MM-DD.  \n",
    "# Earliest valid data for TERIFIC was 2021-12-12, but there were some in-air \n",
    "# measurements before\n",
    "mission_startdate = '2021-12-12'\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# CHOOSE VARIABLE NAMES\n",
    "#----------------------------------------------------------------------------\n",
    "# Check the Slocum master data list 8.2 for a range of options\n",
    "# ISSUE: This was a bit of hit-or-miss to find out which variables existed\n",
    "var_physics = ['sci_water_pressure', 'sci_water_temp', 'sci_water_cond',\n",
    "            'derived_salinity', 'derived_potential_density', 'derived_potential_temperature',\n",
    "           ]\n",
    "\n",
    "var_other = ['m_final_water_vx', 'm_final_water_vy',\n",
    "             'm_final_water_vx_at_surface',\n",
    "             'm_final_water_vy_at_surface',\n",
    "             'm_water_vx', 'm_water_vy',\n",
    "             'm_gps_lon', 'm_gps_lat',\n",
    "             'm_lat', 'm_lon',\n",
    "            ]\n",
    "\n",
    "var_oxy = ['sci_oxy4_oxygen',\n",
    "           'sci_oxy4_calphase',\n",
    "           'sci_oxy4_temp',\n",
    "          ]\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# CHOOSE VARIABLE NAMES - here's the hit-or-miss part.  \n",
    "# Try everything and see what sticks!\n",
    "#----------------------------------------------------------------------------\n",
    "# ISSUE: Need to know which variables actually exist in the dataset, and only\n",
    "# pick the right ones.  Otherwise, this might slowdown the download.\n",
    "# Wetlabs on Unit_398: these parameters seem to work (bb2flsv9)\n",
    "# Wetlabs on unit_409: these parameters seem to work (flbbcd)\n",
    "# POSSIBLE TO JUST ADD MORE VARIABLES AND THEY WILL BE REMOVED LATER\n",
    "# BUT PROBABLY SLOWS THINGS DOWN...\n",
    "var_bio = ['sci_bb2flsv9_b532_scaled', # units ug/l  - blue?? or green\n",
    "            'sci_bb2flsv9_b700_scaled', # units ug/l - red\n",
    "            'sci_bb2flsv9_chl_scaled', # units ug/l\n",
    "            'sci_bb2flsv9_b532_sig', # units ug/l  - blue?? or green\n",
    "            'sci_bb2flsv9_b700_sig', # units ug/l - red\n",
    "            'sci_bb2flsv9_chl_sig', # units ug/l\n",
    "            'sci_bb2flsv9_b532_ref', # units ug/l  - blue?? or green\n",
    "            'sci_bb2flsv9_b700_ref', # units ug/l - red\n",
    "            'sci_bb2flsv9_chl_ref', # units ug/l\n",
    "            'sci_flbbcd_cdom_units', # ppb - 409\n",
    "            'sci_flbbcd_chlor_units', # ug/l\n",
    "            'sci_flbbcd_bb_units', # ??? is this blue backscatter?\n",
    "            'sci_flbbcd_cdom_scaled', # ppb - 409\n",
    "            'sci_flbbcd_chlor_scaled', # ug/l\n",
    "            'sci_flbbcd_bb_scaled', # ??? is this blue backscatter?\n",
    "            'sci_flbbcd_cdom_sig', # ppb - 409\n",
    "            'sci_flbbcd_chlor_sig', # ug/l\n",
    "            'sci_flbbcd_bb_sig', # ??? is this blue backscatter?\n",
    "          ]\n",
    "\n",
    "# Some details for the attributes in the netcdf file.\n",
    "platform_type = 'slocum' # Must be in this format to work with the API\n",
    "project_name = 'TERIFIC'\n",
    "institution_name = 'National Oceanography Centre, UK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789d5d34-7756-4822-b955-b5236bbd8913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Changing idive, idx was 330 is now  329\n",
      "51. Changing iclimb, idx was 2932, is now 2933\n",
      "56. Changing idive, idx was 3355 is now  3354\n",
      "140. Changing idive, idx was 28836 is now  28835\n",
      "157. Changing iclimb, idx was 33897, is now 33898\n",
      "161. Changing iclimb, idx was 35153, is now 35154\n",
      "162. Changing iclimb, idx was 35516, is now 35517\n",
      "202. Changing idive, idx was 47886 is now  47885\n",
      "275. Changing iclimb, idx was 70150, is now 70151\n",
      "285. Changing iclimb, idx was 73200, is now 73201\n",
      "302. Changing iclimb, idx was 78491, is now 78492\n",
      "353. Changing iclimb, idx was 94536, is now 94537\n",
      "365. Changing iclimb, idx was 98577, is now 98578\n",
      "380. Changing iclimb, idx was 102790, is now 102791\n",
      "432. Changing iclimb, idx was 118860, is now 118861\n",
      "438. Changing iclimb, idx was 120881, is now 120882\n",
      "7. Changing iclimb, idx was 366, is now 367\n",
      "54. Changing idive, idx was 3298 is now  3297\n",
      "68. Changing iclimb, idx was 5878, is now 5879\n",
      "222. Changing iclimb, idx was 40325, is now 40326\n",
      "229. Changing iclimb, idx was 42847, is now 42848\n",
      "236. Changing iclimb, idx was 45386, is now 45387\n",
      "240. Changing iclimb, idx was 46699, is now 46700\n",
      "299. Changing iclimb, idx was 66260, is now 66261\n",
      "302. Changing iclimb, idx was 66886, is now 66887\n",
      "332. Changing iclimb, idx was 76952, is now 76953\n",
      "342. Changing iclimb, idx was 80089, is now 80090\n",
      "409. Changing iclimb, idx was 101128, is now 101129\n",
      "412. Changing iclimb, idx was 101850, is now 101851\n",
      "415. Changing iclimb, idx was 102573, is now 102574\n",
      "420. Changing iclimb, idx was 103480, is now 103481\n",
      "424. Changing iclimb, idx was 104275, is now 104276\n",
      "435. Changing idive, idx was 106457 is now  106456\n",
      "449. Changing iclimb, idx was 109851, is now 109852\n",
      "452. Changing iclimb, idx was 110568, is now 110569\n",
      "462. Changing iclimb, idx was 112965, is now 112966\n",
      "471. Changing iclimb, idx was 115121, is now 115122\n",
      "485. Changing iclimb, idx was 118660, is now 118661\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Should not need to edit below here\n",
    "# ==============================================================================\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Check that output directories exist - if not, then exit\n",
    "# ISSUE: Is there a better way to do this with error handling? [EFW]\n",
    "#----------------------------------------------------------------------------\n",
    "outpath = cat_interim_path('')\n",
    "if not os.path.isdir(outpath):\n",
    "    print('Directory not found: '+outpath+'    -- EXITING')\n",
    "    sys.exit(1)\n",
    "outpath = cat_proc_path('')\n",
    "if not os.path.isdir(outpath):\n",
    "    print('Directory not found: '+outpath+'    -- EXITING')\n",
    "    sys.exit(1)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Token and API choices/formatting.\n",
    "#----------------------------------------------------------------------------\n",
    "# https://api.c2.noc.ac.uk/charon/tokens/issue\n",
    "#\n",
    "# Need to copy and paste the token you generated by logging in at the website\n",
    "# above into the file 02-code/myToken.txt\n",
    "with open(\"myToken.txt\", \"r\") as myfile:\n",
    "    myToken = myfile.read().replace('\\n', '')\n",
    "\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "headers = CaseInsensitiveDict()\n",
    "headers[\"Accept\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = f'Bearer {myToken}'\n",
    "\n",
    "# List of glider serial numbers for API\n",
    "unit_list = [(k) for k in glider_names.keys()]\n",
    "\n",
    "# URL for the data\n",
    "api_root = 'https://api.c2.noc.ac.uk/'\n",
    "\n",
    "# Platform type for API\n",
    "platform = platform_type\n",
    "\n",
    "# Format the time string\n",
    "time_strf = '%Y%m%d'\n",
    "\n",
    "# Used to chop data before this date\n",
    "tstart = pd.Timestamp(mission_startdate+'T00')\n",
    "\n",
    "# Change this to a later value to download only a subset of the data\n",
    "download_startdate = mission_startdate+'T00%3A00%3A00'\n",
    "\n",
    "# Date created (for attributes in netcdf file)\n",
    "date_created = dt.datetime.now().strftime(time_strf)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# API choices specific for data (not positioning)\n",
    "#----------------------------------------------------------------------------\n",
    "# Choice of API website for glider sensor data: timeseries/\n",
    "api_choice = 'timeseries/observations/'\n",
    "\n",
    "# Specify format for downloaded file:\n",
    "# Using csv_combined_transposed rather than csv_combined since it seems to \n",
    "# help with getting all the data (not just when the wetlabs was on)\n",
    "format_choice = 'csv_combined_transposed?'\n",
    "\n",
    "# Format the variable list for the API\n",
    "var_list = var_physics+var_bio+var_oxy+var_other\n",
    "var_str = ''\n",
    "for i in var_list:\n",
    "    var_str = var_str+'variable='+i+'&'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d50eb4-1354-44e5-97b9-09d8b0f71fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------\n",
    "# SCIENCE DATA DOWNLOAD: \n",
    "# Loop through the individual gliders & download glider data\n",
    "# - Format the time variable\n",
    "# - Change units on pressure from bar to dbar\n",
    "# - Remove negative derived_salinity values\n",
    "#--------------------------------------------------------------\n",
    "for uname in unit_list:\n",
    "    #--------------------------------------------------------------\n",
    "    # Format the request\n",
    "    #--------------------------------------------------------------\n",
    "    if 0:\n",
    "        # Time limited - can use this to make the dataset smaller when\n",
    "        # testing changes.  Just use a later value for 'dstart'\n",
    "        opt_str = f'from={download_startdate}&'\\\n",
    "        f'{var_str}platform_type={platform}'\\\n",
    "        f'&platform_serial{uname}&reverse_order=false&skip_nulls=false'\\\n",
    "        '&cached=false'\n",
    "        start_yyyymmdd = '_'+str.replace(dstart[0:10],'-','')\n",
    "        \n",
    "    # No time limiting - Download everything\n",
    "    opt_str = f'{var_str}platform_type={platform}'\\\n",
    "    f'&platform_serial={uname}&reverse_order=false&skip_nulls=false'\\\n",
    "    '&cached=false'\n",
    "    start_yyyymmdd = ''\n",
    "\n",
    "    # Concatenate request string\n",
    "    url = api_root+api_choice+format_choice+opt_str\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    # Request the data - save as text in variable 'resp'\n",
    "    #--------------------------------------------------------------\n",
    "    resp = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check the response code \n",
    "    # (200 is good.  If you get something else, token may need refreshing)\n",
    "    if not resp.status_code==200:\n",
    "        print(uname+' - [ resp '+str(resp.status_code)+' ] '\\\n",
    "              'Cannot access data - May need to refresh token? or check URL variable')\n",
    "    else:\n",
    "        print(uname+' - [ resp '+str(resp.status_code)+' ] '\\\n",
    "              'Good response code - parsing data')\n",
    "\n",
    "        # Parse the 'resp' string into a dataFrame\n",
    "        aa = resp.content.decode(\"utf-8\") \n",
    "        data_df = pd.read_csv(StringIO(aa)) # Get rid of the leading b\n",
    "        data_df = data_df.sort_values(['timestamp']) # Sort by time\n",
    " \n",
    "        # Print a little table to the screen\n",
    "        #    print(data_df.head(3))\n",
    "\n",
    "        #--------------------------------------------------------------\n",
    "        # Clean up time format and convert pressure units to dbar\n",
    "        #--------------------------------------------------------------\n",
    "        data_df['time'] = data_df.timestamp.apply(lambda x:\n",
    "                                    dt.datetime.fromtimestamp(x*0.001))\n",
    "        data_df = data_df.drop(columns='timestamp')\n",
    "        # Cut data to post deployment\n",
    "        data_df_2021 = data_df[data_df.time>=tstart].copy()\n",
    "\n",
    "        # Change pressure from bars to dbars\n",
    "        data_df_2021['pressure_dbar'] =  data_df_2021.sci_water_pressure * 10\n",
    "\n",
    "        # Remove negative salinities\n",
    "        df1 = data_df_2021['derived_salinity']\n",
    "        df2 = df1.where(df1>0)\n",
    "        data_df_2021['derived_salinity'] = df2\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------\n",
    "        # Format the output file name (and path in ../01-data/01-raw/\n",
    "        #--------------------------------------------------------------\n",
    "        # Prepare to convert to xarray\n",
    "        data_df2 = data_df_2021\n",
    "        data_df2 = data_df2.set_index(\"time\")\n",
    "        data_df2 = data_df2.drop(columns=\"sci_water_pressure\")\n",
    "\n",
    "        # Convert to xarray\n",
    "        ds_2021 = data_df2.to_xarray()\n",
    "\n",
    "        # Set some attributes\n",
    "        maxtimestr = pd.to_datetime(\n",
    "            ds_2021.time.values.max()).strftime(time_strf)\n",
    "\n",
    "        # Create a dictionary of attributes\n",
    "        attr_dict = {\"Platform\": platform,\n",
    "                     \"End Time\": maxtimestr,\n",
    "                     \"Project\": project_name,\n",
    "                     \"Institution\": institution_name,\n",
    "                     \"Date created\": date_created, \n",
    "                     \"Serial number\": uname,\n",
    "                     \"Platform name\": glider_names[uname],\n",
    "                }\n",
    "\n",
    "        ds_2021 = ds_2021.assign_attrs(attr_dict)\n",
    "\n",
    "        #--------------------------------------------------------------\n",
    "        # Clean up Xarray dataset:\n",
    "        # Remove data ararys that have no non-nan values\n",
    "        #--------------------------------------------------------------\n",
    "        for varname in ds_2021.keys():\n",
    "            data_val = ds_2021[varname].values\n",
    "            num = np.count_nonzero(~np.isnan(data_val))\n",
    "            if num==0:\n",
    "                ds_2021 = ds_2021.drop(varname)\n",
    "                \n",
    "        # Save a netcdf file\n",
    "        outfile = uname+'_'+maxtimestr+'_data.nc'\n",
    "        outfile_with_path = cat_raw_path(outfile)\n",
    "\n",
    "        ds_2021.to_netcdf(outfile_with_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317a69f-faf5-455a-be99-17a0fe5f6126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------\n",
    "# GLIDER POSITIONS DOWNLOAD: \n",
    "# Loop through the individual gliders & download glider tracks\n",
    "#--------------------------------------------------------------\n",
    "# API website to use\n",
    "api_choice = 'positions/'\n",
    "# Data format to request\n",
    "format_choice = 'positions?'\n",
    "\n",
    "# Loop through the glider list\n",
    "for uname in unit_list:\n",
    "    #--------------------------------------------------------------\n",
    "    # Format the request\n",
    "    #--------------------------------------------------------------\n",
    "    if 0:\n",
    "        # Time limited\n",
    "        opt_str = f'from={dstart}&platform_type={platform}'\\\n",
    "        f'&platform_serial={uname}&source_type=internal&time_order=descending'\n",
    "        start_yyyymmdd = '_'+str.replace(dstart[0:10],'-','')\n",
    "    \n",
    "    # No time limiting\n",
    "    opt_str = f'platform_type={platform}'\\\n",
    "    f'&platform_serial={uname}&source_type=internal&time_order=descending'\n",
    "    start_yyyymmdd = ''\n",
    "\n",
    "    # Concatenate request string\n",
    "    url = api_root+api_choice+format_choice+opt_str\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    # Request the data - save as text in variable 'resp'\n",
    "    #--------------------------------------------------------------\n",
    "    resp = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check the response code \n",
    "    # (200 is good.  If you get something else, token may need refreshing)\n",
    "    if not resp.status_code==200:\n",
    "        print(uname+' - [ resp '+str(resp.status_code)+' ] '\\\n",
    "              'Cannot access positions - May need to refresh token? Or check the URL variable')\n",
    "    else:\n",
    "        print(uname+' - [ resp '+str(resp.status_code)+' ] '\\\n",
    "              'Good response code - parsing positions')\n",
    "\n",
    "        # Parse the string into a dataFrame\n",
    "        json_string = resp.content.decode(\"utf-8\") # Get rid of the leading b\n",
    "        bb = ast.literal_eval(json_string)[0]\n",
    "        data_df = pd.DataFrame(bb['positions']['internal'])\n",
    "        data_df.head()\n",
    "\n",
    "        #--------------------------------------------------------------\n",
    "        # Format the output file name (and path in ../01-data/01-raw/\n",
    "        #--------------------------------------------------------------\n",
    "        # Prepare to convert to xarray\n",
    "        data_df2 = data_df\n",
    "        data_df2[\"time\"] = data_df[\"time\"].astype('datetime64').dt.round('1s')\n",
    "        data_df2[\"time_received\"] = data_df[\"time\"].astype('datetime64').dt.round('1s')\n",
    "        data_df2 = data_df2.set_index(\"time\")\n",
    "        data_df2 = data_df2.drop(columns=\"source\")\n",
    "        ds_pos = data_df2.to_xarray()\n",
    "\n",
    "        # Latest date in position file.\n",
    "        maxtimestr = pd.to_datetime(\n",
    "            ds_pos.time.values.max()).strftime(time_strf)\n",
    "\n",
    "       # Create a dictionary of attributes\n",
    "        attr_dict = {\"Platform\": platform+' glider',\n",
    "                     \"End Time\": maxtimestr,\n",
    "                     \"Project\": project_name,\n",
    "                     \"Institution\": institution_name,\n",
    "                     \"Date created\": date_created, \n",
    "                     \"Serial number\": uname,\n",
    "                      \"Platform name\": glider_names[uname],\n",
    "                }\n",
    "\n",
    "\n",
    "        ds_pos = ds_pos.assign_attrs(attr_dict)\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    # Format the output file name (and path in ../01-data/01-raw/\n",
    "    #--------------------------------------------------------------\n",
    "    outfile = uname+'_'+maxtimestr+'_position.nc'\n",
    "    outfile_with_path = cat_raw_path(outfile)\n",
    "\n",
    "    ds_pos.to_netcdf(outfile_with_path, 'w')\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_c2slocum",
   "language": "python",
   "name": "env_c2slocum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
