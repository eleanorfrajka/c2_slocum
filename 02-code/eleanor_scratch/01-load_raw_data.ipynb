{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d638f532",
   "metadata": {},
   "source": [
    "# Load raw data and save as a netcdf\n",
    "\n",
    "- Change the output file format.  This could go straight into an xarray/netcdf rather than a pickle\n",
    "- Change the download of timeseries/ data to only load an update to the time series, and then merge with  data that was already downloaded.  Should be faster\n",
    "- Add Wetlabs data to be downloaded: Not currently downloading.\n",
    " \n",
    "As of 27 Jan 2022: Combined what 01-load_raw_data.ipynb and 02-process-data.ipynb did "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "from io import StringIO\n",
    "import requests\n",
    "from setdir import *\n",
    "import ast # To handle the string conversion when loading json file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50385810-ddb5-42dd-b24b-0306ccc6181b",
   "metadata": {},
   "source": [
    "# User defined options: Gliders and variables, start date\n",
    "\n",
    "## This might be a good place to assign units to variables??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dccd1efc-026d-49b4-bcda-04434819e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slocum gliders: A dictionary with the key as the serial number ('unit_398') \n",
    "# and then the plain text name, \"Churchill\"\n",
    "glider_names = {\n",
    "#    'unit_409': 'Grease',\n",
    "    'unit_398': 'Churchill',\n",
    "}\n",
    "\n",
    "# Choose a start date.  \n",
    "# Earliest valid data for TERIFIC was 2021-12-12, but there were some in-air \n",
    "# measurements before\n",
    "mission_startdate = '2021-12-12'\n",
    "\n",
    "# Choose variable names\n",
    "# Check the Slocum master data list 8.2 for a range of options\n",
    "var_physics = ['sci_water_pressure', 'sci_water_temp', 'sci_water_cond',\n",
    "            'sci_oxy4_oxygen',\n",
    "            'derived_salinity',\n",
    "            'derived_potential_density', 'derived_potential_temperature',\n",
    "            'm_gps_lon', 'm_gps_lat',\n",
    "           ]\n",
    "\n",
    "# Wetlabs on Unit_398: these parameters seem to work (bb2flsv9)\n",
    "wetlabs398 = ['sci_bb2flsv9_b532_scaled', # units ug/l  - blue?? or green\n",
    "            'sci_bb2flsv9_b700_scaled', # units ug/l - red\n",
    "            'sci_bb2flsv9_chl_scaled', # units ug/l\n",
    "           ]\n",
    "\n",
    "# Wetlabs on unit_409: these parameters seem to work (flbbcd)\n",
    "wetlabs409 = ['sci_flbbcd_cdom_units', # ppb - 409\n",
    "              'sci_flbbcd_chlor_units', # ug/l\n",
    "              'sci_flbbcd_bb_units', # ??? is this blue backscatter?\n",
    "             ]\n",
    "\n",
    "\n",
    "# Some details for the attributes in the netcdf file.\n",
    "platform_type = 'slocum' # Must be in this format to work with the API\n",
    "project_name = 'TERIFIC'\n",
    "institution_name = 'National Oceanography Centre, UK'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746ac7d-d12b-485c-900a-c75270e02443",
   "metadata": {},
   "source": [
    "## Get a token\n",
    "\n",
    "https://api.c2.noc.ac.uk/charon/tokens/issue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "192e4c15-0cfd-4d9d-ad3b-263fb6eea975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to copy and paste the token you generated by logging in at the website\n",
    "# above into the file 02-code/myToken.txt\n",
    "with open(\"myToken.txt\", \"r\") as myfile:\n",
    "    myToken = myfile.read().replace('\\n', '')\n",
    "    \n",
    "from requests.structures import CaseInsensitiveDict\n",
    "headers = CaseInsensitiveDict()\n",
    "headers[\"Accept\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = f'Bearer {myToken}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62c799",
   "metadata": {},
   "source": [
    "## API request options for gliders (both data and positioning)\n",
    "\n",
    "For time series data, the website is: https://api.c2.noc.ac.uk/timeseries/doc\n",
    "Check under 'meta/variables' to see what variables can be selected.\n",
    "\n",
    "Be sure to choose platform: slocum and serial number: unit_409 or unit_398\n",
    "\n",
    "\n",
    "\n",
    "An example to download time series data is \n",
    "https://api.c2.noc.ac.uk/timeseries/observations/csv_combined?variable=sci_water_pressure&variable=sci_water_temp&variable=sci_water_cond&variable=sci_oxy4_oxygen&variable=sci_bb2flsv9_chl_scaled&variable=derived_salinity&variable=derived_potential_density&variable=derived_potential_temperature&platform_type=slocum&platform_serial=unit_409&reverse_order=false&skip_nulls=false&cached=false\n",
    "\n",
    "Hmm. removed the wetlabs because it seemed to remove some of the \n",
    "data/discretise it so that not all the T&S came down\n",
    "\n",
    "Note: for positioning data, it's: https://api.c2.noc.ac.uk/positions/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42ba7ef1-d11c-4390-8c78-bc02e4555888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit list for API\n",
    "unit_list = [(k) for k in glider_names.keys()]\n",
    "\n",
    "# URL for the data\n",
    "api_root = 'https://api.c2.noc.ac.uk/'\n",
    "\n",
    "# Platform type for API\n",
    "platform = platform_type\n",
    "\n",
    "# Format the time string\n",
    "time_strf = '%Y%m%d'\n",
    "\n",
    "# Used to chop data before this date\n",
    "tstart = pd.Timestamp(mission_startdate+'T00')\n",
    "\n",
    "# Change this to a later value to download only a subset of the data\n",
    "download_startdate = mission_startdate+'T00%3A00%3A00'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caacc17",
   "metadata": {},
   "source": [
    "# Loop through both gliders to download data\n",
    "\n",
    "Goes through the serial numbers in unit_list and loads the data.  Not sure whether I want to cut the start, but will decide later.\n",
    "Save a netcdf with the data in ../01-data/01-raw/\n",
    "\n",
    "## Includes data-specific API request options\n",
    "\n",
    "- api_choice\n",
    "- var_str\n",
    "\n",
    "\n",
    "### ISSUE: Set up a boolean to decide whether to use a time limited download (from dstart to present) or the full dataset including in-air measurements\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f70c67fd-7d46-4638-811b-0409a0e3e929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit_398[ resp 200 ] Good response code - parsing data\n",
      "unit_409[ resp 200 ] Good response code - parsing data\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# DATA Download: API choices specific for data (not positioning)\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "# Choice of API website:\n",
    "# Not sure how many there are but for glider sensor data, it's timeseries/\n",
    "api_choice = 'timeseries/observations/'\n",
    "\n",
    "# Format for downloaded file:\n",
    "# Using csv_combined_transposed rather than csv_combined since it seems to \n",
    "# help with getting all the data (not just when the wetlabs was on)\n",
    "format_choice = 'csv_combined_transposed?'\n",
    "\n",
    "# Format the variable list for the API\n",
    "var_list = var_physics+wetlabs398+wetlabs409\n",
    "var_str = ''\n",
    "for i in var_list:\n",
    "    var_str = var_str+'variable='+i+'&'\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Some prep before looping\n",
    "#----------------------------------------------------------------------------\n",
    "# Check that the output directory exists\n",
    "# If not, then exit\n",
    "outpath = cat_interim_path('')\n",
    "if not os.path.isdir(outpath):\n",
    "   sys.exit(1)\n",
    "\n",
    "# Date created (for attributes in netcdf file)\n",
    "date_created = datetime.datetime.now().strftime(time_strf)\n",
    "\n",
    "\n",
    "for i in unit_list:\n",
    "    #--------------------------------------------------------------\n",
    "    # Format the request\n",
    "    #--------------------------------------------------------------\n",
    "    if 0:\n",
    "        # Time limited - can use this to make the dataset smaller when\n",
    "        # testing changes.  Just use a later value for 'dstart'\n",
    "        opt_str = f'from={download_startdate}&'\\\n",
    "        f'{var_str}platform_type={platform}'\\\n",
    "        f'&platform_serial{i}&reverse_order=false&skip_nulls=false'\\\n",
    "        '&cached=false'\n",
    "        start_yyyymmdd = '_'+str.replace(dstart[0:10],'-','')\n",
    "        \n",
    "    # No time limiting - Download everything\n",
    "    opt_str = f'{var_str}platform_type={platform}'\\\n",
    "    f'&platform_serial={i}&reverse_order=false&skip_nulls=false'\\\n",
    "    '&cached=false'\n",
    "    start_yyyymmdd = ''\n",
    "\n",
    "    # Concatinate request string\n",
    "    url = api_root+api_choice+format_choice+opt_str\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    # Request the data - save as text in variable 'resp'\n",
    "    #--------------------------------------------------------------\n",
    "    resp = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check the response code \n",
    "    # (200 is good.  If you get something else, token may need refreshing)\n",
    "    if not resp.status_code==200:\n",
    "        print(i+'[ resp '+str(resp.status_code)+' ] '\\\n",
    "              'Cannot access data - May need to refresh token?')\n",
    "        print('Or check the url variable')\n",
    "    else:\n",
    "        print(i+'[ resp '+str(resp.status_code)+' ] '\\\n",
    "              'Good response code - parsing data')\n",
    "\n",
    "        # Parse the 'resp' string into a dataFrame\n",
    "        aa = resp.content.decode(\"utf-8\") \n",
    "        data_df = pd.read_csv(StringIO(aa)) # Get rid of the leading b\n",
    "        data_df = data_df.sort_values(['timestamp']) # Sort by time\n",
    " \n",
    "        # Print a little table to the screen\n",
    "        #    print(data_df.head(3))\n",
    "\n",
    "        #--------------------------------------------------------------\n",
    "        # Clean up time format and convert pressure units to dbar\n",
    "        #--------------------------------------------------------------\n",
    "        data_df['time'] = data_df.timestamp.apply(lambda x:\n",
    "                                    datetime.datetime.fromtimestamp(x*0.001))\n",
    "        data_df = data_df.drop(columns='timestamp')\n",
    "        # Cut data to post deployment\n",
    "        data_df_2021 = data_df[data_df.time>=tstart].copy()\n",
    "\n",
    "        # Change pressure from bars to dbars\n",
    "        data_df_2021['pressure_dbar'] =  data_df_2021.sci_water_pressure * 10\n",
    "\n",
    "        # Remove negative salinities\n",
    "        df1 = data_df_2021['derived_salinity']\n",
    "        df2 = df1.where(df1>0)\n",
    "        data_df_2021['derived_salinity'] = df2\n",
    "        \n",
    "        # ISSUE: Remove any variable where there are no valid values (all nan)\n",
    "\n",
    "        #--------------------------------------------------------------\n",
    "        # Format the output file name (and path in ../01-data/01-raw/\n",
    "        #--------------------------------------------------------------\n",
    "        # Prepare to convert to xarray\n",
    "        data_df2 = data_df_2021\n",
    "        data_df2 = data_df2.set_index(\"time\")\n",
    "        data_df2 = data_df2.drop(columns=\"sci_water_pressure\")\n",
    "\n",
    "        # Convert to xarray\n",
    "        ds_2021 = data_df2.to_xarray()\n",
    "\n",
    "        # Set some attributes\n",
    "        maxtimestr = pd.to_datetime(ds_2021.time.values.max()).strftime(time_strf)\n",
    "\n",
    "        # Create a dictionary of attributes\n",
    "        attr_dict = {\"Platform\": platform,\n",
    "                     \"End Time\": maxtimestr,\n",
    "                     \"Project\": project_name,\n",
    "                     \"Institution\": institution_name,\n",
    "                     \"Date created\": date_created, \n",
    "                     \"Serial number\": i,\n",
    "                     \"Platform name\": glider_names[i],\n",
    "                }\n",
    "\n",
    "        ds_2021 = ds_2021.assign_attrs(attr_dict)\n",
    "\n",
    "        # Save a netcdf file\n",
    "        outfile = i+'_'+maxtimestr+'.nc'\n",
    "        outfile_with_path = cat_raw_path(outfile)\n",
    "\n",
    "        ds_2021.to_netcdf(outfile_with_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623771d",
   "metadata": {},
   "source": [
    "# Get the position data separately\n",
    "\n",
    "Example of a request\n",
    "https://api.c2.noc.ac.uk/positions/positions?platform_type=slocum&platform_serial=unit_409&source_type=internal&time_order=descending\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3deb3586-2a13-48d0-a668-6a3704b26fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit_398[ resp 200 ] Good response code - parsing data\n",
      "unit_409[ resp 200 ] Good response code - parsing data\n"
     ]
    }
   ],
   "source": [
    "# Choose which of the api websites to use.. Not sure how many there are but \n",
    "# for these data, it's timeseries/\n",
    "api_choice = 'positions/'\n",
    "format_choice = 'positions?'\n",
    "\n",
    "for i in unit_list:\n",
    "    if 0:\n",
    "        # Time limited\n",
    "        opt_str = f'from={dstart}&platform_type={platform}'\\\n",
    "        f'&platform_serial={i}&source_type=internal&time_order=descending'\n",
    "        start_yyyymmdd = '_'+str.replace(dstart[0:10],'-','')\n",
    "    \n",
    "    # No time limiting\n",
    "    opt_str = f'platform_type={platform}'\\\n",
    "    f'&platform_serial={i}&source_type=internal&time_order=descending'\n",
    "    start_yyyymmdd = ''\n",
    "\n",
    "    # Concatenate request string\n",
    "    url = api_root+api_choice+format_choice+opt_str\n",
    "\n",
    "    # Get the data\n",
    "    resp = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check the response code \n",
    "    # (200 is good.  If you get something else, token may need refreshing)\n",
    "    if not resp.status_code==200:\n",
    "        print(i+'[ resp '+str(resp.status_code)+' ] '\\\n",
    "              'Cannot access data - May need to refresh token?')\n",
    "        print('Or check the url variable')\n",
    "    else:\n",
    "        print(i+'[ resp '+str(resp.status_code)+' ] '\\\n",
    "              'Good response code - parsing data')\n",
    "\n",
    "        # Parse the string into a dataFrame\n",
    "        json_string = resp.content.decode(\"utf-8\") # Get rid of the leading b\n",
    "        bb = ast.literal_eval(json_string)[0]\n",
    "        data_df = pd.DataFrame(bb['positions']['internal'])\n",
    "        data_df.head()\n",
    "\n",
    "        #--------------------------------------------------------------\n",
    "        # Format the output file name (and path in ../01-data/01-raw/\n",
    "        #--------------------------------------------------------------\n",
    "        # Prepare to convert to xarray\n",
    "        data_df2 = data_df\n",
    "        data_df2[\"time\"] = data_df[\"time\"].astype('datetime64').dt.round('1s')\n",
    "        data_df2[\"time_received\"] = data_df[\"time\"].astype('datetime64').dt.round('1s')\n",
    "        data_df2 = data_df2.set_index(\"time\")\n",
    "        data_df2 = data_df2.drop(columns=\"source\")\n",
    "        ds_pos = data_df2.to_xarray()\n",
    "\n",
    "        maxtimestr = pd.to_datetime(ds_pos.time.values.max()).strftime(time_strf)\n",
    "\n",
    "\n",
    "       # Create a dictionary of attributes\n",
    "        attr_dict = {\"Platform\": platform+' glider',\n",
    "                     \"End Time\": maxtimestr,\n",
    "                     \"Project\": project_name,\n",
    "                     \"Institution\": institution_name,\n",
    "                     \"Date created\": date_created, \n",
    "                     \"Serial number\": i,\n",
    "                      \"Platform name\": glider_names[i],\n",
    "\n",
    "\n",
    "                }\n",
    "\n",
    "\n",
    "        ds_pos = ds_pos.assign_attrs(attr_dict)\n",
    "\n",
    "\n",
    "    # Save a netcdf file\n",
    "    outfile = i+'_position_'+maxtimestr+'.nc'\n",
    "    outfile_with_path = cat_raw_path(outfile)\n",
    "\n",
    "    ds_pos.to_netcdf(outfile_with_path, 'w')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_c2slocum",
   "language": "python",
   "name": "env_c2slocum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
